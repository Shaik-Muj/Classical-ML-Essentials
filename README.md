# Classical ML Essentials

This repository is a curated collection of essential machine learning notebooks designed for aspiring ML engineers and data scientists. It covers the full classical ML pipeline‚Äîfrom preprocessing and feature engineering to model training, evaluation, and optimization.

Each notebook is structured for clarity, interview readiness, and practical implementation using real datasets.

## üìÅ Structure

- **01_Supervised_Learning**  
  Implements and compares core classifiers: Logistic Regression, KNN, SVM, Na√Øve Bayes, Random Forest, and XGBoost.  
  Includes ROC-AUC, F1 Score, and k-Fold Cross-Validation.

- **02_Unsupervised_Learning**  
  Covers K-Means clustering and PCA for dimensionality reduction.  
  Includes Elbow Method and explained variance analysis.

- **03_Preprocessing_and_Tuning**  
  Advanced feature engineering on Titanic dataset: Title extraction, binning, interaction terms.  
  Feature selection via SelectKBest and RFE.  
  Hyperparameter tuning using GridSearchCV and RandomizedSearchCV.

- **04_Regression **  
  Will include Linear Regression, Ridge, Lasso, and tree-based regressors with MSE, RMSE, and R¬≤ evaluation.

## Tech Stack

Python, scikit-learn, pandas, numpy, matplotlib, seaborn, XGBoost

## Purpose

This repo is built to help ML engineers master classical techniques before moving on to deep learning, NLP, or advanced pipelines. It‚Äôs ideal for interviews, portfolio building, and foundational understanding.

---

Feel free to fork, clone, or contribute.
